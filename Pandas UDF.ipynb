{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Arrow in Pyspark\n",
    "Spark可以使用`Apache Arrow`对python和jvm之间的数据进行传输， 这样会比默认传输方式更加高效。\n",
    "为了能高效地利用特性和保障兼容性，使用的时候可能需要一点点修改或者配置。\n",
    "\n",
    "## 为什么使用Arrow作为数据交换中介能够提升性能？\n",
    "普通的python udf需要经过如下步骤来和jvm交互：\n",
    "- jvm中一条数据序列化\n",
    "- 序列化的数据发送到python进程\n",
    "- 记录被python反序列化\n",
    "- 记录被python处理\n",
    "- 结果被python序列化\n",
    "- 结果被发送到jvm\n",
    "- jvm反序列化并存储结果到dataframe\n",
    "\n",
    "所以python udf会比java和scala原生的udf慢。\n",
    "但是使用pandas udf可以克服数据传输中需要的序列化问题，关键是使用了Arrow. spark使用arrow把JVM中的Dataframe转为可共享的buffer, 然后python也可以把这块共享buffer作为pandas的dataframe, 所以python可以直接在共享内存上操作。\n",
    "以上，我们总结一下，使用arrow主要有两个好处：\n",
    "1. 因为直接使用了共享内存，不在需要python和jvm序列化和反序列化数据。\n",
    "2. pandas有很多使用c实现的方法， 可以直接使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame和Pandas DataFrame的转化\n",
    "首先需要配置spark, 设置`spark.sql.execution.arrow.pyspark.enabled`, 默认这个选项是不打开的。\n",
    "还可以开启`spark.sql.execution.arrow.pyspark.fallback.enabled`来避免如果没有安装`Arrow`或者其它相关错误。\n",
    "Spark可以使用`toPandas()`方法转化为Pandas DataFrame(这个方法会单机生成，有性能问题）; 而使用`createDataFrame(pandas_df)`把Pandas DataFrame转为Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"app1\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.916010</td>\n",
       "      <td>0.922255</td>\n",
       "      <td>0.596190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.128835</td>\n",
       "      <td>0.186798</td>\n",
       "      <td>0.236069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.560804</td>\n",
       "      <td>0.828434</td>\n",
       "      <td>0.056276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.245575</td>\n",
       "      <td>0.437656</td>\n",
       "      <td>0.358806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.643864</td>\n",
       "      <td>0.652498</td>\n",
       "      <td>0.775339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.025681</td>\n",
       "      <td>0.746733</td>\n",
       "      <td>0.531121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.068925</td>\n",
       "      <td>0.182059</td>\n",
       "      <td>0.719574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.329564</td>\n",
       "      <td>0.891549</td>\n",
       "      <td>0.175906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.970990</td>\n",
       "      <td>0.360034</td>\n",
       "      <td>0.852293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.138032</td>\n",
       "      <td>0.807608</td>\n",
       "      <td>0.849570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.595900</td>\n",
       "      <td>0.995683</td>\n",
       "      <td>0.931402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.133281</td>\n",
       "      <td>0.549850</td>\n",
       "      <td>0.521694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.178599</td>\n",
       "      <td>0.049187</td>\n",
       "      <td>0.541581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.497080</td>\n",
       "      <td>0.828292</td>\n",
       "      <td>0.688637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.360940</td>\n",
       "      <td>0.738829</td>\n",
       "      <td>0.924751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.882976</td>\n",
       "      <td>0.690087</td>\n",
       "      <td>0.010570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.246459</td>\n",
       "      <td>0.450924</td>\n",
       "      <td>0.224788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.564636</td>\n",
       "      <td>0.748374</td>\n",
       "      <td>0.072823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.288805</td>\n",
       "      <td>0.517276</td>\n",
       "      <td>0.263859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.515909</td>\n",
       "      <td>0.922622</td>\n",
       "      <td>0.795256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.916010  0.922255  0.596190\n",
       "1   0.128835  0.186798  0.236069\n",
       "2   0.560804  0.828434  0.056276\n",
       "3   0.245575  0.437656  0.358806\n",
       "4   0.643864  0.652498  0.775339\n",
       "5   0.025681  0.746733  0.531121\n",
       "6   0.068925  0.182059  0.719574\n",
       "7   0.329564  0.891549  0.175906\n",
       "8   0.970990  0.360034  0.852293\n",
       "9   0.138032  0.807608  0.849570\n",
       "10  0.595900  0.995683  0.931402\n",
       "11  0.133281  0.549850  0.521694\n",
       "12  0.178599  0.049187  0.541581\n",
       "13  0.497080  0.828292  0.688637\n",
       "14  0.360940  0.738829  0.924751\n",
       "15  0.882976  0.690087  0.010570\n",
       "16  0.246459  0.450924  0.224788\n",
       "17  0.564636  0.748374  0.072823\n",
       "18  0.288805  0.517276  0.263859\n",
       "19  0.515909  0.922622  0.795256"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(20, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "result_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas UDF\n",
    "`Pandas UDF`是用户定义的函数， Spark是用arrow传输数据并用pandas来运行`pandas UDF`， `pandas UDF`使用向量计算，相比于旧版本的`row-at-a-time`python udf, 最多增加100倍的性能. 使用`pandas_udf`修饰器装饰函数，就可以定义一个`pandas UDF`.对spark来说，UDF就是一个普通的pyspark函数。\n",
    "从spark3.0开始， 推荐使用python类型(`type hint`)来定义pandas udf.\n",
    "定义类型的时候，`StructType`需要使用`pandas.DataFrame`类型， 其他一律使用`pandas.Series`类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- long_col: long (nullable = true)\n",
      " |-- string_col: string (nullable = true)\n",
      " |-- struct_col: struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      " |    |-- col2: long (nullable = true)\n",
      "\n",
      "+--------------------------------------+\n",
      "|func(long_col, string_col, struct_col)|\n",
      "+--------------------------------------+\n",
      "|                  [constant string, 9]|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3:pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col1'] = 'constant string'\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [[1, 'a string', ('a nested string',)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1: string>\"\n",
    ")\n",
    "df.printSchema()\n",
    "df.select(func('long_col', 'string_col', 'struct_col')).printSchema()\n",
    "df.select(func('long_col', 'string_col', 'struct_col')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义Series to Series UDF\n",
    "当类型提示可以被表达为`pandas.Series -> pandas.Series`时，称为`Series to Series`UDF\n",
    "这种类型的`pandas UDF`的输入和输出必须要有相同的长度， PySpark会把数据按列分成多个batch, 然后对每个batch运行`pandas UDF`, 然后组合各自的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  9|\n",
      "|                 16|\n",
      "|                 25|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "def multiply_func(a: pd.Series, b:pd.Series) -> pd.Series:\n",
    "    return a*b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
    "x = pd.Series([1,3,4,5])\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=['x']))\n",
    "df.select(multiply(col('x'), col('x'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义Series迭代器 -> Series迭代器 UDF\n",
    "当类型提示可以被表达为`Iterator[pandas.Series] -> Iterator[pandas.Series]`时，称为`Iterator[Series] to Iterator[Series]`UDF. 这种类型在原有一个python函数接受迭代器传参时可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|plus_one(x)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          4|\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "pdf = pd.DataFrame([1,3,4], columns=['x'])\n",
    "df = spark.createDataFrame(pdf)\n",
    "@pandas_udf('long')\n",
    "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for x in iterator:\n",
    "        yield x+1\n",
    "\n",
    "df.select(plus_one('x')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义多个Series迭代器 -> Series迭代器的UDF\n",
    "当类型提示可以被表达为`Iterator[Tuple[pandas.Series,...]] -> Iterator[pandas.Series]`时，称为`Iterator[Tuple[pandas.Series,...]] to Iterator[Series]`UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|multiply_two_cols(x, x)|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      9|\n",
      "|                     16|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "@pandas_udf('long')\n",
    "def multiply_two_cols(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    for a,b in iterator:\n",
    "        yield a*b\n",
    "\n",
    "df.select(multiply_two_cols('x','x')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义Series -> Scalar类型的UDF\n",
    "当类型提示可以被表达为`pandas.Series -> Scalar`时，称为`Series to Scalar`UDF.\n",
    "Scalar具体的类型必须是原生python类型如int, float等等， 或者是numpy的数据类型如numpy.int64, numpy.float64\n",
    "这种UDF可以被用于`groupBy(), agg(), pyspark.sql.Window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|mean_udf(v)|\n",
      "+-----------+\n",
      "|        4.0|\n",
      "+-----------+\n",
      "\n",
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|  5.6666665|\n",
      "+---+-----------+\n",
      "\n",
      "+---+----+------+\n",
      "| id|   v|mean_v|\n",
      "+---+----+------+\n",
      "|  1| 1.0|   1.5|\n",
      "|  1| 2.0|   2.0|\n",
      "|  2| 3.0|   3.5|\n",
      "|  2| 4.0|   7.0|\n",
      "|  2|10.0|  10.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "df = spark.createDataFrame([(1,1.0), (1,2.0), (2,3.0),(2,4.0), (2,10.0)], ('id', 'v'))\n",
    "@pandas_udf('float')\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "df.select(mean_udf('v')).show()\n",
    "df.groupby('id').agg(mean_udf('v')).show()\n",
    "\n",
    "# 在window的范围内做平均\n",
    "w = Window.partitionBy('id').rowsBetween(Window.currentRow, 1)\n",
    "df.withColumn('mean_v', mean_udf('v').over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
